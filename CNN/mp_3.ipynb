{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Problem 3: NumPy CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy import signal\n",
    "from imageio import imread\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_images\n",
    "    # Read in images and makes a list for each set in the form: [images, labels]\n",
    "    # images: np array with dims [N x img_height x img width x num_channels]\n",
    "    # labels: np array with dims [N x 1]. elephant = 0, lionfish = 1\n",
    "    #\n",
    "    # Returns:  train_set: The list [train_images, train_labels]\n",
    "    #           val_set: The list [val_images, val_labels] \n",
    "\n",
    "def load_images():\n",
    "    \n",
    "    sets = ['train', 'val']\n",
    "    \n",
    "    data_sets = []\n",
    "    for dset in sets:\n",
    "        img_path = './bin_dataset/' + dset + '/ele'\n",
    "        ele_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        img_path = './bin_dataset/' + dset + '/lio'\n",
    "        lio_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        set_images = np.stack(ele_list + lio_list)\n",
    "        N = set_images.shape[0]\n",
    "        labels = np.ones((N,1))\n",
    "        labels[0:int(N/2)] = 0\n",
    "        data_sets.append([set_images, labels])\n",
    "\n",
    "    train_set, val_set = data_sets\n",
    "\n",
    "    print(\"Loaded\", len(train_set[0]), \"training images\")\n",
    "    print(\"Loaded\", len(val_set[0]), \"validation images\")\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "# batchify\n",
    "    # Inputs:    train_set: List containing images and labels\n",
    "    #            batch size: The desired size of each batch\n",
    "    #\n",
    "    # Returns:   image_batches: A list of shuffled training image batches, each with size batch_size\n",
    "    #            label_batches: A list of shuffled training label batches, each with size batch_size \n",
    "\n",
    "def batchify(train_set, batch_size):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    data_size,_,_,_ = train_set[0].shape\n",
    "    x = list(range(data_size))\n",
    "    shuffle(x);\n",
    "    k = 0\n",
    "    image_batches = []\n",
    "    label_batches = []\n",
    "    for i in range(int(data_size/batch_size)):\n",
    "        temp_train_list = []\n",
    "        temp_label_list = []\n",
    "        for j in range(batch_size):\n",
    "            val = x[k]\n",
    "            temp_train_list.append(np.array(train_set[0][val]))\n",
    "            temp_label_list.append(np.array(train_set[1][val]))\n",
    "            k += 1\n",
    "        image_batches.append(np.array(temp_train_list))\n",
    "        label_batches.append(np.array(temp_label_list))\n",
    "    return image_batches, label_batches\n",
    "# def batchify(train_set, batch_size):\n",
    "    \n",
    "#     # YOUR CODE HERE\n",
    "#     # initialized two lists\n",
    "#     image_batches = []\n",
    "#     label_batches = []\n",
    "    \n",
    "#     shuffle_index = np.arange(len(train_set[0]))\n",
    "#     shuffle(shuffle_index)\n",
    "    \n",
    "#     image_chunk = [None] * batch_size\n",
    "#     label_chunk = [None] * batch_size\n",
    "#     for c in range(0, len(shuffle_index), batch_size):\n",
    "#         for i in range(batch_size):\n",
    "#             image_chunk[i] = train_set[0][shuffle_index[c+i]]\n",
    "#             label_chunk[i] = train_set[1][shuffle_index[c+i]]\n",
    "#         image_batches.append(np.array(image_chunk))\n",
    "#         label_batches.append(np.array(label_chunk))\n",
    "\n",
    "#     return image_batches, label_batches\n",
    "\n",
    "\n",
    "\n",
    "def normalization(train_set, val_set):\n",
    "    new_train = train_set.astype(float)\n",
    "    new_train /= 256.0\n",
    "    new_val = val_set.astype(float)\n",
    "    new_val /= 256.0\n",
    "    return new_train, new_val;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "    # Inputs:   x: Multi-dimensional array with size N along the first axis\n",
    "    # \n",
    "    # Returns:  out: Multi-dimensional array with same size of x \n",
    "\n",
    "def relu(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    out = np.maximum(0,x);\n",
    "    return out\n",
    "\n",
    "\n",
    "# sigmoid\n",
    "    # Inputs:    x: Multi-dimensional array with size N along the first axis\n",
    "    # \n",
    "    # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    out = 1.0/(1.0+ np.exp(-x))\n",
    "    return out\n",
    "\n",
    "\n",
    "# unit_step\n",
    "    # Inputs:    x: Multi-dimensional array with size N along the first axis \n",
    "    # \n",
    "    # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "def unit_step(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    out = np.heaviside(x,1);\n",
    "    return out \n",
    "# relu\n",
    "    # Inputs:   x: Multi-dimensional array with size N along the first axis\n",
    "    # \n",
    "    # Returns:  out: Multi-dimensional array with same size of x \n",
    "\n",
    "# def relu(x):\n",
    "    \n",
    "#     # YOUR CODE HERE\n",
    "#     out = np.maximum(x, 0)\n",
    "#     return out\n",
    "\n",
    "\n",
    "# # sigmoid\n",
    "#     # Inputs:    x: Multi-dimensional array with size N along the first axis\n",
    "#     # \n",
    "#     # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "# def sigmoid(x):\n",
    "    \n",
    "#     # YOUR CODE HERE\n",
    "#     out = 1.0/(1 + np.exp(-x))\n",
    "#     return out\n",
    "\n",
    "# def d_sigmoid(x):\n",
    "#     out = sigmoid(x) * (1-sigmoid(x))\n",
    "#     return out\n",
    "\n",
    "# # unit_step\n",
    "#     # Inputs:    x: Multi-dimensional array with size N along the first axis \n",
    "#     # \n",
    "#     # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "# def unit_step(x):\n",
    "    \n",
    "#     # YOUR CODE HERE\n",
    "#     out = np.heaviside(x, 1)\n",
    "#     return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve2D\n",
    "    # Inputs:    X: [N x height x width x num_channels]\n",
    "    #            filters: [num_filters x filter_height x filter_width x num_input_channels]\n",
    "    # \n",
    "    # Returns:   Xc: output array by convoling X and filters. [N x output_height x output_width x num_filters]\n",
    "\n",
    "def convolve2D(X0, filters):\n",
    "   \n",
    "    N, X0_len, _, num_ch = X0.shape\n",
    "    num_out_ch, filter_len, _, _ = filters.shape\n",
    "    F0_side = X0_len - filter_len + 1\n",
    "    \n",
    "    F0 = np.zeros((N, F0_side, F0_side, num_out_ch))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for o_ch in range(num_out_ch):\n",
    "            for ch in range(num_ch):\n",
    "                # YOUR CODE HERE\n",
    "                F0[n,0:,0:,o_ch] += signal.convolve2d(X0[n,0:,0:,ch],filters[o_ch,0:,0:,ch],mode = \"valid\")\n",
    "    return F0\n",
    "\n",
    "\n",
    "# maxPool\n",
    "    # Inputs:    R0: [N x height x width x num_channels]\n",
    "    #            mp_len: size of max pool window, also the stride for this MP\n",
    "    # \n",
    "    # Returns:   p_out: output of pooling R0. [N x output_height x output_width x num_channels]\n",
    "    #            R0_mask: A binary mask with the same size as R0. Indicates which index was chosen to be the max\n",
    "    #            for each max pool window. This will be used for backpropagation.\n",
    "\n",
    "def maxPool(R0, mp_len):\n",
    "\n",
    "    N, R0_len, _, num_ch = R0.shape\n",
    "    p_out_len = int((R0_len-mp_len)/mp_len + 1)\n",
    "\n",
    "    R0_mask = np.zeros(R0.shape)\n",
    "    p_out = np.zeros((N, p_out_len, p_out_len, num_ch))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for ch in range(num_ch):\n",
    "            for row in range(p_out_len): \n",
    "                for col in range(p_out_len):\n",
    "                    # YOUR CODE HERE\n",
    "                    u_row = row*mp_len\n",
    "                    d_row = min(R0_len,(row+1)*mp_len)\n",
    "                    l_col = col*mp_len\n",
    "                    r_col = min(R0_len,(col+1)*mp_len)\n",
    "                    temp_arr = R0[n,u_row:d_row,l_col:r_col,ch]\n",
    "                    p_out[n,row,col,ch] = np.amax(temp_arr)\n",
    "                    temp_index = np.argmax(temp_arr)\n",
    "                    temp_r = temp_index//mp_len + u_row\n",
    "                    temp_c = temp_index%mp_len + l_col\n",
    "                    R0_mask[n,temp_r,temp_c,ch] = 1\n",
    "    return p_out, R0_mask\n",
    "\n",
    "\n",
    "# flatten\n",
    "    #input:      X1'[N x height x width x num_channels]\n",
    "    #\n",
    "    #\n",
    "    #Return:     X1 [N x (height x width x num_channels)]\n",
    "def flatten(X1_p):\n",
    "    N,h,w,ch = X1_p.shape\n",
    "    X1 = X1_p.reshape((N,(h*w*ch)))\n",
    "    return X1\n",
    "    \n",
    "\n",
    "# fc\n",
    "    # Inputs:    X: [N x num_input_features]\n",
    "    #            W: [num_input_features x num_fc_nodes]\n",
    "    # \n",
    "    # Returns:   out: Linear combination of X and W. [N x num_fc_nodes]\n",
    "    \n",
    "def fc(X, W):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    out = X@W\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_fwd\n",
    "    # Inputs:    X0: batch of images. [N x img_height x img_width x num_channels]\n",
    "    #            W0, W1, W2: Parameters of the CNN\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    # \n",
    "    # Returns:   sig: vector containing the output for each sample. [N x 1]\n",
    "    #            cache: a dict containing the relevant output layer calculations that will be\n",
    "    #            used in backpropagation\n",
    "    \n",
    "def cnn_fwd(X0, W0, W1, W2, mp_len):\n",
    "    \n",
    "    # F0 \n",
    "    # YOUR CODE HERE\n",
    "    F0 = convolve2D(X0,W0)\n",
    "    R0 = relu(F0)\n",
    "    # X1p \n",
    "    # YOUR CODE HERE\n",
    "    X1p,R0_mask = maxPool(R0, mp_len)\n",
    "    # X1 (flatten)\n",
    "    # YOUR CODE HERE\n",
    "    X1 = flatten(X1p)\n",
    "    # FC Layers\n",
    "    # YOUR CODE HERE\n",
    "    F1 = fc(X1,W1)\n",
    "    X2 = relu(F1)\n",
    "    F2 = fc(X2,W2)\n",
    "    sig = sigmoid(F2)\n",
    "    # Output\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Save outputs of functions for backward pass\n",
    "    cache = {\n",
    "        \"F0\":F0,\n",
    "        \"R0\":R0,\n",
    "        \"X1p\":X1p,\n",
    "        \"R0m\":R0_mask,\n",
    "        \"X1\":X1,\n",
    "        \"F1\":F1,\n",
    "        \"X2\":X2,\n",
    "        \"F2\":F2,\n",
    "        \"sig\":sig\n",
    "    }\n",
    "    \n",
    "    return sig, cache\n",
    "\n",
    "\n",
    "# loss\n",
    "    # Inputs:    sig: vector containing the CNN output for each sample. [N x 1]\n",
    "    #            Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    # \n",
    "    # Returns:   L: Loss/error criterion for the model. \n",
    "\n",
    "def loss(sig, Y):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # cross entropy\n",
    "    L = 0.0\n",
    "    N, _ = Y.shape\n",
    "    for i in range(N):\n",
    "        L -= Y[i]*np.log(sig[i]) + (1-Y[i])*np.log(1-sig[i])\n",
    "    L /= N\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve2DBwd\n",
    "    # Inputs:    X0: batch of images. [N x height x width x num_channels]\n",
    "    #            dL_dF0: Gradient at the output of the conv layer. \n",
    "    # \n",
    "    # Returns:   dL_dW0. gradient of loss L wrt W0. Same size as W0\n",
    "\n",
    "def convolve2DBwd(X0, dL_dF0):\n",
    "    \n",
    "    N, X0_len, _, num_ch = X0.shape\n",
    "    _, dL_dF0_len, _, num_out_ch  = dL_dF0.shape\n",
    "    filter_len = X0_len - dL_dF0_len + 1\n",
    "    \n",
    "    dL_dW0 = np.zeros((num_out_ch, filter_len, filter_len, num_ch))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for o_ch in range(num_out_ch):\n",
    "            for ch in range(num_ch):\n",
    "                # YOUR CODE HERE \n",
    "                dL_dW0[o_ch,:,:,ch] += signal.convolve2d(X0[n,::-1,::-1,ch], dL_dF0[n,:,:,o_ch], mode=\"valid\")\n",
    "    return dL_dW0\n",
    "\n",
    "\n",
    "# maxPoolBwd\n",
    "    # Inputs:    dL_dX1p: Gradient at the output of the MaxPool layer\n",
    "    #            R0_mask: A binary mask with the same size as R0. Defined in maxPool\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    # \n",
    "    # Returns:   dL_dR0: Gradient at the output of ReLu\n",
    "    \n",
    "def maxPoolBwd(dL_dX1p, R0_mask,  mp_len):\n",
    "    \n",
    "    N, H, W, C = R0_mask.shape\n",
    "    N, dH, dW, C = dL_dX1p.shape\n",
    "    \n",
    "    dL_dR0 = np.zeros(R0_mask.shape)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for ch in range(C):\n",
    "            for row in range(dH):\n",
    "                for col in range(dW):\n",
    "                    # YOUR CODE HERE\n",
    "                    u_row = row*mp_len\n",
    "                    d_row = (row+1)*mp_len\n",
    "                    l_col = col*mp_len\n",
    "                    r_col = (col+1)*mp_len\n",
    "                    temp_arr = R0_mask[n,u_row:d_row,l_col:r_col,ch]\n",
    "                    dL_dR0[n,u_row:d_row,l_col:r_col,ch] = dL_dX1p[n,row,col,ch]*temp_arr\n",
    "    return dL_dR0\n",
    "\n",
    "\n",
    "\n",
    "# def maxPoolBwd(dL_dX1p, R0_mask,  mp_len):\n",
    "    \n",
    "#     N, H, W, C = R0_mask.shape\n",
    "#     N, dH, dW, C = dL_dX1p.shape\n",
    "    \n",
    "#     dL_dR0 = np.zeros(R0_mask.shape)\n",
    "    \n",
    "#     for n in range(N):\n",
    "#         for ch in range(C):\n",
    "#             for row in range(dH):\n",
    "#                 for col in range(dW):\n",
    "#                     # YOUR CODE HERE\n",
    "#                     dL_dR0[n,row*mp_len:(row+1)*mp_len, col*mp_len:(col+1)*mp_len,ch] = dL_dX1p[n,row,col,ch]*R0_mask[n,row*mp_len:(row+1)*mp_len, col*mp_len:(col+1)*mp_len,ch]\n",
    "#     return dL_dR0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dL_dF2(Y, cache):\n",
    "    N,_ = Y.shape\n",
    "    sig = cache[\"sig\"]\n",
    "    ret = (sig - Y) / N\n",
    "    return ret # N x 1\n",
    "\n",
    "\n",
    "\n",
    "# dL_dW2\n",
    "    # Inputs:    Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW2: Gradient of the Loss wrt W2\n",
    "    \n",
    "def dL_dW2(Y, cache):\n",
    "   \n",
    "    # YOUR CODE HERE\n",
    "    X2 = cache[\"X2\"]\n",
    "    dL_F2 = dL_dF2(Y, cache)\n",
    "    dL_dW2 = X2.T@dL_F2\n",
    "    return dL_dW2 # 2 x 1\n",
    "\n",
    "\n",
    "# dL_dW1\n",
    "    # Inputs:    Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            W2: Weight matrix for the second FC layer\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW1: Gradient of the Loss wrt W1\n",
    "    \n",
    "def dL_dW1(cache, dF1):\n",
    "    X1 = cache[\"X1\"]\n",
    "    dL_dW1 = X1.T @ dF1\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return dL_dW1 # 192 x 2\n",
    "\n",
    "def dL_dX2(dF2, W2):\n",
    "    return dF2@W2.T # N x 2\n",
    "\n",
    "def dL_dF1(dX2,cache):\n",
    "    F1 = cache[\"F1\"]\n",
    "    return dX2*unit_step(F1) # N x 2\n",
    "def dL_dX1(dF1, W1):\n",
    "    return dF1 @ W1.T\n",
    "\n",
    "def flattenBwd(dX1, cache):\n",
    "    X1p = cache[\"X1p\"]\n",
    "    N,h,w,ch = X1p.shape\n",
    "    X1p_bwd = dX1.reshape((N,h,w,ch))\n",
    "    return X1p_bwd # N x 8 x 8 x 3\n",
    "\n",
    "def dL_dF0(dR0,cache):\n",
    "    F0 = cache[\"F0\"]\n",
    "    return dR0*unit_step(F0) # n x 96 x 96 x 3\n",
    "    \n",
    "# dL_dW0\n",
    "    # Inputs:    X0: batch of images. [N x height x width x num_channels]\n",
    "    #            Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            W1: Weight matrix for the first FC layer\n",
    "    #            W2: Weight matrix for the second FC layer\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW0: Gradient of the Loss wrt W0\n",
    "\n",
    "# def dL_dW0(X0, Y, W1, W2, mp_len, cache):\n",
    "#     N, X1p_len, _, no_out_ch  = cache['X1p'].shape\n",
    "#     F2 = cache['F2']\n",
    "#     F1 = cache['F1']\n",
    "#     R0m = cache['R0m']\n",
    "#     F0 = cache['F0']\n",
    "    \n",
    "#     #dL_dF2\n",
    "#     # YOUR CODE HERE\n",
    "#     dL_F2 = dL_dF2(Y,cache)\n",
    "#     dL_\n",
    "#     #dL_dF1\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     #dL_dX1\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # dL_dX1p (unflatten)\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # dL_dR0 (unpool)\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # dL_dF0 (relu_bwd)\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # dL_dW0\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     return dL_dW0\n",
    "def cnn_bwd(X0,Y,W1,W2,mp_len,cache):\n",
    "    R0_mask = cache[\"R0m\"]\n",
    "    dF2 = dL_dF2(Y, cache)\n",
    "    dW2 = dL_dW2(Y,cache)\n",
    "    dX2 = dL_dX2(dF2, W2)\n",
    "    dF1 = dL_dF1(dX2,cache)\n",
    "    dW1 = dL_dW1(cache, dF1)\n",
    "    dX1 = dL_dX1(dF1, W1)\n",
    "    dX1p = flattenBwd(dX1, cache)\n",
    "    dR0 = maxPoolBwd(dX1p, R0_mask,  mp_len)\n",
    "    dF0 = dL_dF0(dR0,cache)\n",
    "    dW0 = convolve2DBwd(X0, dF0)\n",
    "    return dW0,dW1,dW2\n",
    "        \n",
    "def update(W0, W1, W2, dW0, dW1, dW2, lr):\n",
    "    W0_p = W0 - lr*dW0\n",
    "    W1_p = W1 - lr*dW1\n",
    "    W2_p = W2 - lr*dW2\n",
    "    return W0_p, W1_p, W2_p\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 training images\n",
      "Loaded 800 validation images\n"
     ]
    }
   ],
   "source": [
    "# Load images and scale them\n",
    "# YOUR CODE HERE\n",
    "train_set, val_set = load_images()\n",
    "train_set[0], val_set[0] = normalization(train_set[0], val_set[0])\n",
    "# image_batches, label_batches = batchify(train_set,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "lr = 0.1\n",
    "image_size = 10\n",
    "batch_size = 16\n",
    "filter_len = 5\n",
    "num_out_ch = 3\n",
    "mp_len = 12\n",
    "fc_nodes = 2\n",
    "# Declare weights\n",
    "# YOUR CODE HERE\n",
    "W0 = np.random.normal(0,0.05,(num_out_ch,filter_len,filter_len,3))\n",
    "W1 = np.random.normal(0,0.05,(192, fc_nodes))\n",
    "W2 = np.random.normal(0, 0.05, (fc_nodes, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1\n",
      "train_loss: [0.71425516] train_acc: 0.50375\n",
      "---total cost for this epoch is 33.12486481666565 seconds ---\n",
      "current learning rate is: 0.1\n",
      "After epoch 2\n",
      "train_loss: [0.53120658] train_acc: 0.7525\n",
      "---total cost for this epoch is 32.700456857681274 seconds ---\n",
      "current learning rate is: 0.1\n",
      "After epoch 3\n",
      "train_loss: [0.31803687] train_acc: 0.86\n",
      "---total cost for this epoch is 32.39314293861389 seconds ---\n",
      "current learning rate is: 0.1\n",
      "After epoch 4\n",
      "train_loss: [0.54807283] train_acc: 0.6425\n",
      "---total cost for this epoch is 32.54948401451111 seconds ---\n",
      "current learning rate is: 0.1\n",
      "After epoch 5\n",
      "train_loss: [0.33883] train_acc: 0.85625\n",
      "---total cost for this epoch is 32.32273507118225 seconds ---\n",
      "current learning rate is: 0.1\n",
      "After epoch 6\n",
      "train_loss: [0.31233813] train_acc: 0.86375\n",
      "---total cost for this epoch is 33.08328914642334 seconds ---\n",
      "current learning rate is: 0.1\n",
      "After epoch 7\n",
      "train_loss: [0.27580261] train_acc: 0.88625\n",
      "---total cost for this epoch is 33.956066846847534 seconds ---\n",
      "current learning rate is: 0.07500000000000001\n",
      "After epoch 8\n",
      "train_loss: [0.33113253] train_acc: 0.86875\n",
      "---total cost for this epoch is 32.733619928359985 seconds ---\n",
      "current learning rate is: 0.07500000000000001\n",
      "After epoch 9\n",
      "train_loss: [0.32554034] train_acc: 0.86375\n",
      "---total cost for this epoch is 32.55116319656372 seconds ---\n",
      "current learning rate is: 0.07500000000000001\n",
      "After epoch 10\n",
      "train_loss: [0.29019007] train_acc: 0.8725\n",
      "---total cost for this epoch is 32.55331015586853 seconds ---\n",
      "current learning rate is: 0.07500000000000001\n",
      "After epoch 11\n",
      "train_loss: [0.26068576] train_acc: 0.89375\n",
      "---total cost for this epoch is 32.560445070266724 seconds ---\n",
      "current learning rate is: 0.07500000000000001\n",
      "After epoch 12\n",
      "train_loss: [0.26800306] train_acc: 0.8925\n",
      "---total cost for this epoch is 32.44883608818054 seconds ---\n",
      "current learning rate is: 0.05625000000000001\n",
      "After epoch 13\n",
      "train_loss: [0.26181319] train_acc: 0.89375\n",
      "---total cost for this epoch is 32.50240182876587 seconds ---\n",
      "current learning rate is: 0.05625000000000001\n",
      "After epoch 14\n",
      "train_loss: [0.25954011] train_acc: 0.89375\n",
      "---total cost for this epoch is 32.56057691574097 seconds ---\n",
      "current learning rate is: 0.05625000000000001\n",
      "After epoch 15\n",
      "train_loss: [0.25567364] train_acc: 0.9025\n",
      "---total cost for this epoch is 32.50262689590454 seconds ---\n",
      "current learning rate is: 0.05625000000000001\n",
      "After epoch 16\n",
      "train_loss: [0.29431429] train_acc: 0.8825\n",
      "---total cost for this epoch is 32.57184290885925 seconds ---\n",
      "current learning rate is: 0.05625000000000001\n",
      "After epoch 17\n",
      "train_loss: [0.25246324] train_acc: 0.90125\n",
      "---total cost for this epoch is 32.62466096878052 seconds ---\n",
      "current learning rate is: 0.0421875\n",
      "After epoch 18\n",
      "train_loss: [0.24834092] train_acc: 0.905\n",
      "---total cost for this epoch is 32.78866505622864 seconds ---\n",
      "current learning rate is: 0.0421875\n",
      "After epoch 19\n",
      "train_loss: [0.27054327] train_acc: 0.8925\n",
      "---total cost for this epoch is 32.59430813789368 seconds ---\n",
      "current learning rate is: 0.0421875\n",
      "After epoch 20\n",
      "train_loss: [0.30947543] train_acc: 0.8825\n",
      "---total cost for this epoch is 32.55331993103027 seconds ---\n",
      "current learning rate is: 0.0421875\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "error = []\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # make set of batches\n",
    "    # YOUR CODE HERE\n",
    "    image_batches, label_batches = batchify(train_set, batch_size)\n",
    "    num_batches = len(image_batches)\n",
    "    \n",
    "    # record the time for the execution\n",
    "    start_time = time.time()\n",
    "    for b_idx in range(num_batches):\n",
    "        X = image_batches[b_idx]\n",
    "        Y = label_batches[b_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        # YOUR CODE HERE\n",
    "        sig, cache = cnn_fwd(X, W0, W1, W2, mp_len)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        # YOUR CODE HERE\n",
    "        dW0, dW1, dW2 = cnn_bwd(X, Y, W1, W2, mp_len, cache)\n",
    "        \n",
    "        # Update gradients\n",
    "        # YOUR CODE HERE\n",
    "        W0, W1, W2 = update(W0, W1, W2, dW0, dW1, dW2, lr)\n",
    "        \n",
    "    # calcuate the loss and accuracy in each epoch\n",
    "    sig, _ = cnn_fwd(val_set[0], W0, W1, W2, mp_len)\n",
    "    train_acc = len(np.where(np.round(sig) == val_set[1])[0])/len(val_set[1])\n",
    "    accuracy.append(train_acc)\n",
    "    err = loss(sig, val_set[1])\n",
    "    error.append(err)\n",
    "   #\n",
    "    if (i % 5) == 0 and i > 4:\n",
    "        lr *= 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy vs Epoches')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = np.arange(epochs)\n",
    "plt.plot(index, accuracy, 'r-')\n",
    "plt.xlabel(\"Epoches\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Epoches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Correctness of Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "train_loss: [0.23266417] train_acc: 0.9175\n"
     ]
    }
   ],
   "source": [
    "weights = np.load('weights.npz')\n",
    "W0 = weights['W0']\n",
    "W1 = weights['W1']\n",
    "W2 = weights['W2']\n",
    "print(W2.shape)\n",
    "sig, _ = cnn_fwd(val_set[0], W0, W1, W2, mp_len)\n",
    "train_acc = len(np.where(np.round(sig) == val_set[1])[0])/len(val_set[1])\n",
    "\n",
    "print(\"train_loss:\", loss(sig, val_set[1]), \"train_acc:\", train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 value: -881.56533203125\n",
      "W1 value: -499.5653320312501\n",
      "W0 value: -658.5653320312501\n"
     ]
    }
   ],
   "source": [
    "# Make backprop testing batch\n",
    "X_bp = np.vstack([train_set[0][0:8,:,:,:], train_set[0][-9:-1,:,:,:]])\n",
    "Y_bp = np.vstack([train_set[1][0:8], train_set[1][-9:-1]])\n",
    "\n",
    "# Initialize weights to all ones\n",
    "# YOUR CODE HERE\n",
    "W0 = np.ones_like(weights['W0'])\n",
    "W1 = np.ones_like(weights['W1'])\n",
    "W2 = np.ones_like(weights['W2'])\n",
    "# Update weights once\n",
    "# YOUR CODE HERE\n",
    "sig, cache = cnn_fwd(X_bp, W0, W1, W2, mp_len)\n",
    "dW0, dW1, dW2 = cnn_bwd(X_bp, Y_bp, W1, W2, mp_len, cache)\n",
    "W0, W1, W2 = update(W0, W1, W2, dW0, dW1, dW2, lr)\n",
    "print(\"W2 value:\", np.sum(W2))\n",
    "print(\"W1 value:\", np.sum(W1))\n",
    "print(\"W0 value:\", np.sum(W0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
